<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - filtered.info - sources/inference/TensorRTInferencer.cpp</title>
  <link rel="stylesheet" type="text/css" href="../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../index.html">top level</a> - <a href="index.html">sources/inference</a> - TensorRTInferencer.cpp<span style="font-size: 80%;"> (source / <a href="TensorRTInferencer.cpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">filtered.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">124</td>
            <td class="headerCovTableEntry">162</td>
            <td class="headerCovTableEntryMed">76.5 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2025-08-19 17:07:46</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">8</td>
            <td class="headerCovTableEntry">11</td>
            <td class="headerCovTableEntryLo">72.7 %</td>
          </tr>
          <tr><td><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : #include &quot;../../includes/inference/TensorRTInferencer.hpp&quot;  // Include TensorRTInferencer header</a>
<a name="2"><span class="lineNum">       2 </span>            : #include &lt;fstream&gt;</a>
<a name="3"><span class="lineNum">       3 </span>            : #include &lt;iostream&gt;</a>
<a name="4"><span class="lineNum">       4 </span>            : #include &lt;stdexcept&gt;</a>
<a name="5"><span class="lineNum">       5 </span>            : #include &lt;numeric&gt;</a>
<a name="6"><span class="lineNum">       6 </span>            : </a>
<a name="7"><span class="lineNum">       7 </span>            : // Constructor: loads TensorRT engine, allocates memory and sets up execution context</a>
<a name="8"><span class="lineNum">       8 </span><span class="lineCov">         24 : TensorRTInferencer::TensorRTInferencer(const std::string&amp; enginePath) :</span></a>
<a name="9"><span class="lineNum">       9 </span>            :         runtime(nullptr),        // Initialize runtime pointer to nullptr</a>
<a name="10"><span class="lineNum">      10 </span>            :         engine(nullptr),         // Initialize engine pointer to nullptr</a>
<a name="11"><span class="lineNum">      11 </span>            :         context(nullptr),        // Initialize execution context pointer to nullptr</a>
<a name="12"><span class="lineNum">      12 </span>            :         inputBindingIndex(-1),   // Initialize input binding index</a>
<a name="13"><span class="lineNum">      13 </span>            :         outputBindingIndex(-1),  // Initialize output binding index</a>
<a name="14"><span class="lineNum">      14 </span>            :         inputSize(208, 208),     // Set default input image size</a>
<a name="15"><span class="lineNum">      15 </span>            :         deviceInput(nullptr),    // Initialize device input pointer to nullptr</a>
<a name="16"><span class="lineNum">      16 </span>            :         deviceOutput(nullptr),   // Initialize device output pointer to nullptr</a>
<a name="17"><span class="lineNum">      17 </span>            :         stream(nullptr),         // Initialize CUDA stream pointer to nullptr</a>
<a name="18"><span class="lineNum">      18 </span>            :         hostInput(nullptr),      // Initialize host input pointer to nullptr</a>
<a name="19"><span class="lineNum">      19 </span><span class="lineCov">         31 :         hostOutput(nullptr) {    // Initialize host output pointer to nullptr</span></a>
<a name="20"><span class="lineNum">      20 </span>            : </a>
<a name="21"><span class="lineNum">      21 </span><span class="lineCov">         24 :         cudaSetDevice(0);  // Set CUDA device to GPU 0</span></a>
<a name="22"><span class="lineNum">      22 </span>            : </a>
<a name="23"><span class="lineNum">      23 </span><span class="lineCov">         24 :         engineData = readEngineFile(enginePath);  // Load serialized engine file into memory</span></a>
<a name="24"><span class="lineNum">      24 </span>            : </a>
<a name="25"><span class="lineNum">      25 </span><span class="lineCov">         23 :         runtime = nvinfer1::createInferRuntime(Logger::instance());  // Create TensorRT runtime with logger</span></a>
<a name="26"><span class="lineNum">      26 </span><span class="lineCov">         23 :         if (!runtime) {  // Check if runtime creation failed</span></a>
<a name="27"><span class="lineNum">      27 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Failed to create TensorRT Runtime&quot;);</span></a>
<a name="28"><span class="lineNum">      28 </span>            :         }</a>
<a name="29"><span class="lineNum">      29 </span>            : </a>
<a name="30"><span class="lineNum">      30 </span><span class="lineCov">         23 :         engine = runtime-&gt;deserializeCudaEngine(engineData.data(), engineData.size());  // Deserialize the engine from the loaded data</span></a>
<a name="31"><span class="lineNum">      31 </span><span class="lineCov">         23 :         if (!engine) {  // Check if deserialization failed</span></a>
<a name="32"><span class="lineNum">      32 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Failed to deserialize engine&quot;);</span></a>
<a name="33"><span class="lineNum">      33 </span>            :         }</a>
<a name="34"><span class="lineNum">      34 </span>            : </a>
<a name="35"><span class="lineNum">      35 </span><span class="lineCov">         23 :         context = engine-&gt;createExecutionContext();  // Create execution context from the engine</span></a>
<a name="36"><span class="lineNum">      36 </span><span class="lineCov">         23 :         if (!context) {  // Check if context creation failed</span></a>
<a name="37"><span class="lineNum">      37 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Failed to create execution context&quot;);</span></a>
<a name="38"><span class="lineNum">      38 </span>            :         }</a>
<a name="39"><span class="lineNum">      39 </span>            : </a>
<a name="40"><span class="lineNum">      40 </span><span class="lineCov">         23 :         lanePostProcessor = new LanePostProcessor(150, 150, 10.0f, 10.0f);  // Initialize lane post-processor with parameters</span></a>
<a name="41"><span class="lineNum">      41 </span><span class="lineCov">         23 :         laneCurveFitter = new LaneCurveFitter(); // Initialize lane curve fitter</span></a>
<a name="42"><span class="lineNum">      42 </span>            : </a>
<a name="43"><span class="lineNum">      43 </span><span class="lineCov">         69 :         for (int i = 0; i &lt; engine-&gt;getNbBindings(); i++) {  // Loop through all bindings</span></a>
<a name="44"><span class="lineNum">      44 </span><span class="lineCov">         46 :                 if (engine-&gt;bindingIsInput(i)) {  // If binding is input</span></a>
<a name="45"><span class="lineNum">      45 </span><span class="lineCov">         23 :                         inputBindingIndex = i;  // Save input binding index</span></a>
<a name="46"><span class="lineNum">      46 </span>            :                 } else {</a>
<a name="47"><span class="lineNum">      47 </span><span class="lineCov">         23 :                         outputBindingIndex = i;  // Otherwise, save output binding index</span></a>
<a name="48"><span class="lineNum">      48 </span>            :                 }</a>
<a name="49"><span class="lineNum">      49 </span>            :         }</a>
<a name="50"><span class="lineNum">      50 </span>            : </a>
<a name="51"><span class="lineNum">      51 </span><span class="lineCov">         23 :         if (inputBindingIndex == -1 || outputBindingIndex == -1) {  // Verify both input and output were found</span></a>
<a name="52"><span class="lineNum">      52 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Could not find input and output bindings&quot;);</span></a>
<a name="53"><span class="lineNum">      53 </span>            :         }</a>
<a name="54"><span class="lineNum">      54 </span>            : </a>
<a name="55"><span class="lineNum">      55 </span><span class="lineCov">         23 :         inputDims = engine-&gt;getBindingDimensions(inputBindingIndex);  // Get input tensor dimensions</span></a>
<a name="56"><span class="lineNum">      56 </span><span class="lineCov">         23 :         outputDims = engine-&gt;getBindingDimensions(outputBindingIndex);  // Get output tensor dimensions</span></a>
<a name="57"><span class="lineNum">      57 </span>            : </a>
<a name="58"><span class="lineNum">      58 </span><span class="lineCov">         23 :         if (inputDims.d[0] == -1) {  // If input has dynamic batch dimension</span></a>
<a name="59"><span class="lineNum">      59 </span><span class="lineNoCov">          0 :                 nvinfer1::Dims4 explicitDims(1, inputSize.height, inputSize.width, 1);  // Define explicit batch size and dimensions</span></a>
<a name="60"><span class="lineNum">      60 </span><span class="lineNoCov">          0 :                 context-&gt;setBindingDimensions(inputBindingIndex, explicitDims);  // Set explicit input dimensions</span></a>
<a name="61"><span class="lineNum">      61 </span><span class="lineNoCov">          0 :                 inputDims = context-&gt;getBindingDimensions(inputBindingIndex);  // Update inputDims after setting</span></a>
<a name="62"><span class="lineNum">      62 </span>            :         }</a>
<a name="63"><span class="lineNum">      63 </span>            : </a>
<a name="64"><span class="lineNum">      64 </span><span class="lineCov">         23 :         outputDims = context-&gt;getBindingDimensions(outputBindingIndex);  // Confirm and update outputDims</span></a>
<a name="65"><span class="lineNum">      65 </span>            : </a>
<a name="66"><span class="lineNum">      66 </span><span class="lineCov">        115 :         for (int i = 0; i &lt; outputDims.nbDims; i++) {  // Check if any output dimension is dynamic</span></a>
<a name="67"><span class="lineNum">      67 </span><span class="lineCov">         92 :                 if (outputDims.d[i] &lt; 0) {</span></a>
<a name="68"><span class="lineNum">      68 </span><span class="lineNoCov">          0 :                         throw std::runtime_error(&quot;Output shape is undefined or dynamic&quot;);  // Throw error if output is not fully defined</span></a>
<a name="69"><span class="lineNum">      69 </span>            :                 }</a>
<a name="70"><span class="lineNum">      70 </span>            :         }</a>
<a name="71"><span class="lineNum">      71 </span>            : </a>
<a name="72"><span class="lineNum">      72 </span><span class="lineCov">         23 :         inputElementCount = 1;  // Initialize input element count</span></a>
<a name="73"><span class="lineNum">      73 </span><span class="lineCov">        115 :         for (int i = 0; i &lt; inputDims.nbDims; i++) {  // Multiply all input dimensions</span></a>
<a name="74"><span class="lineNum">      74 </span><span class="lineCov">         92 :                 inputElementCount *= static_cast&lt;size_t&gt;(inputDims.d[i]);</span></a>
<a name="75"><span class="lineNum">      75 </span>            :         }</a>
<a name="76"><span class="lineNum">      76 </span><span class="lineCov">         23 :         inputByteSize = inputElementCount * sizeof(float);  // Calculate input buffer size in bytes</span></a>
<a name="77"><span class="lineNum">      77 </span>            : </a>
<a name="78"><span class="lineNum">      78 </span><span class="lineCov">         23 :         outputElementCount = 1;  // Initialize output element count</span></a>
<a name="79"><span class="lineNum">      79 </span><span class="lineCov">        115 :         for (int i = 0; i &lt; outputDims.nbDims; i++) {  // Multiply all output dimensions</span></a>
<a name="80"><span class="lineNum">      80 </span><span class="lineCov">         92 :                 outputElementCount *= static_cast&lt;size_t&gt;(outputDims.d[i]);</span></a>
<a name="81"><span class="lineNum">      81 </span>            :         }</a>
<a name="82"><span class="lineNum">      82 </span><span class="lineCov">         23 :         outputByteSize = outputElementCount * sizeof(float);  // Calculate output buffer size in bytes</span></a>
<a name="83"><span class="lineNum">      83 </span>            : </a>
<a name="84"><span class="lineNum">      84 </span>            :         cudaError_t status;  // Define variable for checking CUDA errors</a>
<a name="85"><span class="lineNum">      85 </span>            : </a>
<a name="86"><span class="lineNum">      86 </span><span class="lineCov">         23 :         status = cudaStreamCreate(&amp;stream);  // Create a CUDA stream for async operations</span></a>
<a name="87"><span class="lineNum">      87 </span><span class="lineCov">         23 :         if (status != cudaSuccess) {  // Check stream creation</span></a>
<a name="88"><span class="lineNum">      88 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Failed to create CUDA stream: &quot; + std::string(cudaGetErrorString(status)));</span></a>
<a name="89"><span class="lineNum">      89 </span>            :         }</a>
<a name="90"><span class="lineNum">      90 </span>            : </a>
<a name="91"><span class="lineNum">      91 </span><span class="lineCov">         23 :         status = cudaMalloc(&amp;deviceInput, inputByteSize);  // Allocate device memory for input tensor</span></a>
<a name="92"><span class="lineNum">      92 </span><span class="lineCov">         23 :         if (status != cudaSuccess) {  // Check input memory allocation</span></a>
<a name="93"><span class="lineNum">      93 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Failed to allocate input memory on GPU: &quot; + std::string(cudaGetErrorString(status)));</span></a>
<a name="94"><span class="lineNum">      94 </span>            :         }</a>
<a name="95"><span class="lineNum">      95 </span>            : </a>
<a name="96"><span class="lineNum">      96 </span><span class="lineCov">         23 :         status = cudaMalloc(&amp;deviceOutput, outputByteSize);  // Allocate device memory for output tensor</span></a>
<a name="97"><span class="lineNum">      97 </span><span class="lineCov">         23 :         if (status != cudaSuccess) {  // Check output memory allocation</span></a>
<a name="98"><span class="lineNum">      98 </span><span class="lineNoCov">          0 :                 cudaFree(deviceInput);  // Free previously allocated input memory if failed</span></a>
<a name="99"><span class="lineNum">      99 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Failed to allocate output memory on GPU: &quot; + std::string(cudaGetErrorString(status)));</span></a>
<a name="100"><span class="lineNum">     100 </span>            :         }</a>
<a name="101"><span class="lineNum">     101 </span>            : </a>
<a name="102"><span class="lineNum">     102 </span><span class="lineCov">         23 :         bindings.resize(engine-&gt;getNbBindings());  // Resize bindings array to number of bindings</span></a>
<a name="103"><span class="lineNum">     103 </span><span class="lineCov">         23 :         bindings[inputBindingIndex] = deviceInput;  // Assign device input buffer</span></a>
<a name="104"><span class="lineNum">     104 </span><span class="lineCov">         23 :         bindings[outputBindingIndex] = deviceOutput;  // Assign device output buffer</span></a>
<a name="105"><span class="lineNum">     105 </span>            : </a>
<a name="106"><span class="lineNum">     106 </span><span class="lineCov">         23 :         Publisher::instance(5556); // Initialize publisher for inference results</span></a>
<a name="107"><span class="lineNum">     107 </span>            : </a>
<a name="108"><span class="lineNum">     108 </span><span class="lineCov">         23 :         initUndistortMaps();  // Initialize undistortion maps for camera calibration</span></a>
<a name="109"><span class="lineNum">     109 </span><span class="lineCov">         23 :         cudaStream = cv::cuda::Stream();  // CUDA stream for asynchronous operations</span></a>
<a name="110"><span class="lineNum">     110 </span><span class="lineCov">         23 : }</span></a>
<a name="111"><span class="lineNum">     111 </span>            : </a>
<a name="112"><span class="lineNum">     112 </span>            : // Clean up allocated GPU resources (device memory, streams)</a>
<a name="113"><span class="lineNum">     113 </span><span class="lineCov">         23 : void TensorRTInferencer::cleanupResources() {</span></a>
<a name="114"><span class="lineNum">     114 </span><span class="lineCov">         23 :         if (deviceInput) cudaFree(deviceInput);   // Free input buffer if allocated</span></a>
<a name="115"><span class="lineNum">     115 </span><span class="lineCov">         23 :         if (deviceOutput) cudaFree(deviceOutput); // Free output buffer if allocated</span></a>
<a name="116"><span class="lineNum">     116 </span><span class="lineCov">         23 :         if (stream) cudaStreamDestroy(stream);    // Destroy CUDA stream if created</span></a>
<a name="117"><span class="lineNum">     117 </span><span class="lineCov">         23 :         if (lanePostProcessor) delete lanePostProcessor; // Delete post-processor object</span></a>
<a name="118"><span class="lineNum">     118 </span><span class="lineCov">         23 :         if (laneCurveFitter) delete laneCurveFitter; // Delete post-processor and curve fitter objects</span></a>
<a name="119"><span class="lineNum">     119 </span><span class="lineCov">         23 :         deviceInput = nullptr;    // Set pointers to nullptr after freeing</span></a>
<a name="120"><span class="lineNum">     120 </span><span class="lineCov">         23 :         deviceOutput = nullptr;</span></a>
<a name="121"><span class="lineNum">     121 </span><span class="lineCov">         23 :         stream = nullptr;</span></a>
<a name="122"><span class="lineNum">     122 </span><span class="lineCov">         23 : }</span></a>
<a name="123"><span class="lineNum">     123 </span>            : </a>
<a name="124"><span class="lineNum">     124 </span>            : // Destructor: free all allocated resources</a>
<a name="125"><span class="lineNum">     125 </span><span class="lineCov">         23 : TensorRTInferencer::~TensorRTInferencer() {</span></a>
<a name="126"><span class="lineNum">     126 </span><span class="lineCov">         23 :         if (hostInput) cudaFreeHost(hostInput);   // Free pinned host memory for input</span></a>
<a name="127"><span class="lineNum">     127 </span><span class="lineCov">         23 :         if (hostOutput) cudaFreeHost(hostOutput); // Free pinned host memory for output</span></a>
<a name="128"><span class="lineNum">     128 </span><span class="lineCov">         23 :         cleanupResources();  // Free GPU resources</span></a>
<a name="129"><span class="lineNum">     129 </span>            : </a>
<a name="130"><span class="lineNum">     130 </span><span class="lineCov">         23 :         if (context) {</span></a>
<a name="131"><span class="lineNum">     131 </span><span class="lineCov">         23 :                 context-&gt;destroy();   // Destroy TensorRT execution context</span></a>
<a name="132"><span class="lineNum">     132 </span>            :         }</a>
<a name="133"><span class="lineNum">     133 </span><span class="lineCov">         23 :         if (engine) {</span></a>
<a name="134"><span class="lineNum">     134 </span><span class="lineCov">         23 :                 engine-&gt;destroy();    // Destroy TensorRT engine</span></a>
<a name="135"><span class="lineNum">     135 </span>            :         }</a>
<a name="136"><span class="lineNum">     136 </span><span class="lineCov">         23 :         if (runtime) {</span></a>
<a name="137"><span class="lineNum">     137 </span><span class="lineCov">         23 :                 runtime-&gt;destroy();   // Destroy TensorRT runtime</span></a>
<a name="138"><span class="lineNum">     138 </span>            :         }</a>
<a name="139"><span class="lineNum">     139 </span><span class="lineCov">         23 : }</span></a>
<a name="140"><span class="lineNum">     140 </span>            : </a>
<a name="141"><span class="lineNum">     141 </span>            : // Read the serialized TensorRT engine file into memory</a>
<a name="142"><span class="lineNum">     142 </span><span class="lineCov">         24 : std::vector&lt;char&gt; TensorRTInferencer::readEngineFile(const std::string&amp; enginePath) {</span></a>
<a name="143"><span class="lineNum">     143 </span><span class="lineCov">         48 :         std::ifstream file(enginePath, std::ios::binary | std::ios::ate);  // Open file in binary mode, go to end</span></a>
<a name="144"><span class="lineNum">     144 </span><span class="lineCov">         24 :         if (!file.good()) {   // Check if file opened successfully</span></a>
<a name="145"><span class="lineNum">     145 </span><span class="lineCov">          1 :                 throw std::runtime_error(&quot;Engine file not found: &quot; + enginePath);</span></a>
<a name="146"><span class="lineNum">     146 </span>            :         }</a>
<a name="147"><span class="lineNum">     147 </span>            : </a>
<a name="148"><span class="lineNum">     148 </span><span class="lineCov">         23 :         size_t size = file.tellg();  // Get file size</span></a>
<a name="149"><span class="lineNum">     149 </span><span class="lineCov">         23 :         file.seekg(0, std::ios::beg);  // Go back to beginning of file</span></a>
<a name="150"><span class="lineNum">     150 </span>            : </a>
<a name="151"><span class="lineNum">     151 </span><span class="lineCov">         23 :         std::vector&lt;char&gt; buffer(size);  // Create buffer of the correct size</span></a>
<a name="152"><span class="lineNum">     152 </span><span class="lineCov">         23 :         if (!file.read(buffer.data(), size)) {  // Read file into buffer</span></a>
<a name="153"><span class="lineNum">     153 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Failed to read engine file&quot;);</span></a>
<a name="154"><span class="lineNum">     154 </span>            :         }</a>
<a name="155"><span class="lineNum">     155 </span>            : </a>
<a name="156"><span class="lineNum">     156 </span><span class="lineCov">         46 :         return buffer;  // Return loaded engine buffer</span></a>
<a name="157"><span class="lineNum">     157 </span>            : }</a>
<a name="158"><span class="lineNum">     158 </span>            : </a>
<a name="159"><span class="lineNum">     159 </span>            : // Preprocess input image on GPU: convert to grayscale, resize, normalize</a>
<a name="160"><span class="lineNum">     160 </span><span class="lineCov">         28 : cv::cuda::GpuMat TensorRTInferencer::preprocessImage(const cv::cuda::GpuMat&amp; gpuImage) {</span></a>
<a name="161"><span class="lineNum">     161 </span><span class="lineCov">         28 :         if (gpuImage.empty()) {  // Validate input image</span></a>
<a name="162"><span class="lineNum">     162 </span><span class="lineCov">          2 :                 throw std::runtime_error(&quot;Input image is empty&quot;);</span></a>
<a name="163"><span class="lineNum">     163 </span>            :         }</a>
<a name="164"><span class="lineNum">     164 </span>            : </a>
<a name="165"><span class="lineNum">     165 </span><span class="lineCov">         26 :         if (gpuImage.type() != CV_8UC3 &amp;&amp; gpuImage.type() != CV_8UC1) {  // Check if input image is in expected format</span></a>
<a name="166"><span class="lineNum">     166 </span><span class="lineCov">          2 :                 throw std::runtime_error(&quot;Input image must be CV_8UC3 (color) or CV_8UC1 (grayscale)&quot;);</span></a>
<a name="167"><span class="lineNum">     167 </span>            :         }</a>
<a name="168"><span class="lineNum">     168 </span>            : </a>
<a name="169"><span class="lineNum">     169 </span><span class="lineCov">         48 :         cv::cuda::GpuMat gpuGray;</span></a>
<a name="170"><span class="lineNum">     170 </span><span class="lineCov">         24 :         if (gpuImage.channels() &gt; 1) {   // If input has multiple channels (color)</span></a>
<a name="171"><span class="lineNum">     171 </span><span class="lineCov">         22 :                 cv::cuda::cvtColor(gpuImage, gpuGray, cv::COLOR_BGR2GRAY); // Convert to grayscale</span></a>
<a name="172"><span class="lineNum">     172 </span>            :         } else {</a>
<a name="173"><span class="lineNum">     173 </span><span class="lineCov">          2 :                 gpuGray = gpuImage;  // Already grayscale, no conversion needed</span></a>
<a name="174"><span class="lineNum">     174 </span>            :         }</a>
<a name="175"><span class="lineNum">     175 </span>            : </a>
<a name="176"><span class="lineNum">     176 </span><span class="lineCov">         48 :         cv::cuda::GpuMat gpuResized;</span></a>
<a name="177"><span class="lineNum">     177 </span><span class="lineCov">         24 :         cv::cuda::resize(gpuGray, gpuResized, inputSize, 0, 0, cv::INTER_LINEAR); // Resize to network input size</span></a>
<a name="178"><span class="lineNum">     178 </span>            : </a>
<a name="179"><span class="lineNum">     179 </span><span class="lineCov">         24 :         cv::cuda::GpuMat gpuFloat;</span></a>
<a name="180"><span class="lineNum">     180 </span><span class="lineCov">         24 :         gpuResized.convertTo(gpuFloat, CV_32F, 1.0 / 255.0); // Normalize to [0,1] and convert to float32</span></a>
<a name="181"><span class="lineNum">     181 </span>            : </a>
<a name="182"><span class="lineNum">     182 </span><span class="lineCov">         48 :         return gpuFloat;  // Return preprocessed image (still on GPU)</span></a>
<a name="183"><span class="lineNum">     183 </span>            : }</a>
<a name="184"><span class="lineNum">     184 </span>            : </a>
<a name="185"><span class="lineNum">     185 </span>            : // Run inference given a GpuMat input (already preprocessed)</a>
<a name="186"><span class="lineNum">     186 </span><span class="lineCov">         21 : void TensorRTInferencer::runInference(const cv::cuda::GpuMat&amp; gpuInput) {</span></a>
<a name="187"><span class="lineNum">     187 </span><span class="lineCov">         21 :         if (gpuInput.rows != inputSize.height || gpuInput.cols != inputSize.width) {  // Verify input dimensions</span></a>
<a name="188"><span class="lineNum">     188 </span><span class="lineCov">          2 :                 throw std::runtime_error(&quot;Input image dimensions do not match expected dimensions&quot;);</span></a>
<a name="189"><span class="lineNum">     189 </span>            :         }</a>
<a name="190"><span class="lineNum">     190 </span>            : </a>
<a name="191"><span class="lineNum">     191 </span><span class="lineCov">         57 :         cudaError_t err = cudaMemcpy2DAsync(</span></a>
<a name="192"><span class="lineNum">     192 </span>            :                 deviceInput,                          // Destination: TensorRT input buffer</a>
<a name="193"><span class="lineNum">     193 </span><span class="lineCov">         19 :                 inputSize.width * sizeof(float),      // Destination row stride</span></a>
<a name="194"><span class="lineNum">     194 </span><span class="lineCov">         19 :                 gpuInput.ptr&lt;float&gt;(),                // Source pointer: GpuMat data</span></a>
<a name="195"><span class="lineNum">     195 </span><span class="lineCov">         19 :                 gpuInput.step,                        // Source stride</span></a>
<a name="196"><span class="lineNum">     196 </span><span class="lineCov">         19 :                 inputSize.width * sizeof(float),      // Width to copy in bytes</span></a>
<a name="197"><span class="lineNum">     197 </span><span class="lineCov">         19 :                 inputSize.height,                     // Height to copy (rows)</span></a>
<a name="198"><span class="lineNum">     198 </span>            :                 cudaMemcpyDeviceToDevice,             // Type of copy: GPU to GPU</a>
<a name="199"><span class="lineNum">     199 </span>            :                 stream                                // Use CUDA stream</a>
<a name="200"><span class="lineNum">     200 </span>            :         );</a>
<a name="201"><span class="lineNum">     201 </span>            : </a>
<a name="202"><span class="lineNum">     202 </span><span class="lineCov">         19 :         if (err != cudaSuccess) {  // Check if memory copy failed</span></a>
<a name="203"><span class="lineNum">     203 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;cudaMemcpy2DAsync failed: &quot; + std::string(cudaGetErrorString(err)));</span></a>
<a name="204"><span class="lineNum">     204 </span>            :         }</a>
<a name="205"><span class="lineNum">     205 </span>            : </a>
<a name="206"><span class="lineNum">     206 </span>            :         /* if (!context-&gt;enqueueV2(bindings.data(), stream, nullptr)) {  // Enqueue inference on the GPU</a>
<a name="207"><span class="lineNum">     207 </span>            :                 throw std::runtime_error(&quot;TensorRT inference execution failed&quot;);</a>
<a name="208"><span class="lineNum">     208 </span>            :         } */</a>
<a name="209"><span class="lineNum">     209 </span>            : </a>
<a name="210"><span class="lineNum">     210 </span><span class="lineCov">         19 :         context-&gt;executeV2(bindings.data()); // Or executeAsyncV3 if needed</span></a>
<a name="211"><span class="lineNum">     211 </span><span class="lineCov">         19 : }</span></a>
<a name="212"><span class="lineNum">     212 </span>            : </a>
<a name="213"><span class="lineNum">     213 </span>            : // Perform full prediction pipeline: preprocess, inference, and extract output</a>
<a name="214"><span class="lineNum">     214 </span><span class="lineCov">         17 : cv::cuda::GpuMat TensorRTInferencer::makePrediction(const cv::cuda::GpuMat&amp; gpuImage) {</span></a>
<a name="215"><span class="lineNum">     215 </span><span class="lineCov">         34 :         cv::cuda::GpuMat gpuInputFloat = preprocessImage(gpuImage);  // Preprocess input image on GPU</span></a>
<a name="216"><span class="lineNum">     216 </span>            : </a>
<a name="217"><span class="lineNum">     217 </span><span class="lineCov">         17 :         runInference(gpuInputFloat);  // Run inference</span></a>
<a name="218"><span class="lineNum">     218 </span>            : </a>
<a name="219"><span class="lineNum">     219 </span><span class="lineCov">         17 :         int height = outputDims.d[1];</span></a>
<a name="220"><span class="lineNum">     220 </span><span class="lineCov">         17 :         int width  = outputDims.d[2];</span></a>
<a name="221"><span class="lineNum">     221 </span>            : </a>
<a name="222"><span class="lineNum">     222 </span>            :         // Allocate or resize the output mask on GPU if it's not allocated or has wrong size</a>
<a name="223"><span class="lineNum">     223 </span><span class="lineCov">         17 :         if (outputMaskGpu.empty() || outputMaskGpu.rows != height || outputMaskGpu.cols != width) {</span></a>
<a name="224"><span class="lineNum">     224 </span><span class="lineCov">          7 :                 outputMaskGpu = cv::cuda::GpuMat(height, width, CV_32F);</span></a>
<a name="225"><span class="lineNum">     225 </span>            :         }</a>
<a name="226"><span class="lineNum">     226 </span>            : </a>
<a name="227"><span class="lineNum">     227 </span>            :         // Copy the raw prediction output from TensorRT device memory to `outputMaskGpu`</a>
<a name="228"><span class="lineNum">     228 </span>            :         // - Assumes output is already in device memory (deviceOutput)</a>
<a name="229"><span class="lineNum">     229 </span>            :         // - No CPU-GPU transfer, all device-to-device</a>
<a name="230"><span class="lineNum">     230 </span><span class="lineCov">         17 :         cudaMemcpy2DAsync(</span></a>
<a name="231"><span class="lineNum">     231 </span><span class="lineCov">         17 :                 outputMaskGpu.ptr&lt;float&gt;(), outputMaskGpu.step,</span></a>
<a name="232"><span class="lineNum">     232 </span><span class="lineCov">         17 :                 deviceOutput, width * sizeof(float),</span></a>
<a name="233"><span class="lineNum">     233 </span><span class="lineCov">         17 :                 width * sizeof(float), height,</span></a>
<a name="234"><span class="lineNum">     234 </span>            :                 cudaMemcpyDeviceToDevice, stream</a>
<a name="235"><span class="lineNum">     235 </span>            :         );</a>
<a name="236"><span class="lineNum">     236 </span>            : </a>
<a name="237"><span class="lineNum">     237 </span><span class="lineCov">         34 :         return outputMaskGpu;</span></a>
<a name="238"><span class="lineNum">     238 </span>            : }</a>
<a name="239"><span class="lineNum">     239 </span>            : </a>
<a name="240"><span class="lineNum">     240 </span><span class="lineCov">         23 : void TensorRTInferencer::initUndistortMaps() {</span></a>
<a name="241"><span class="lineNum">     241 </span><span class="lineCov">         23 :         cv::Mat cameraMatrix, distCoeffs;</span></a>
<a name="242"><span class="lineNum">     242 </span><span class="lineCov">         46 :         cv::FileStorage fs(&quot;/home/jetson/models/lane-detection/camera_calibration.yml&quot;, cv::FileStorage::READ);  // Open calibration file</span></a>
<a name="243"><span class="lineNum">     243 </span>            : </a>
<a name="244"><span class="lineNum">     244 </span><span class="lineCov">         23 :         if (!fs.isOpened()) {</span></a>
<a name="245"><span class="lineNum">     245 </span><span class="lineNoCov">          0 :                 std::cerr &lt;&lt; &quot;[Error] Failed to open camera_calibration.yml&quot; &lt;&lt; std::endl;</span></a>
<a name="246"><span class="lineNum">     246 </span><span class="lineNoCov">          0 :                 return;  // Handle file opening error</span></a>
<a name="247"><span class="lineNum">     247 </span>            :         }</a>
<a name="248"><span class="lineNum">     248 </span>            : </a>
<a name="249"><span class="lineNum">     249 </span><span class="lineCov">         23 :         fs[&quot;camera_matrix&quot;] &gt;&gt; cameraMatrix;  // Read camera matrix</span></a>
<a name="250"><span class="lineNum">     250 </span><span class="lineCov">         23 :         fs[&quot;distortion_coefficients&quot;] &gt;&gt; distCoeffs;  // Read distortion coefficients</span></a>
<a name="251"><span class="lineNum">     251 </span><span class="lineCov">         23 :         fs.release();  // Close file</span></a>
<a name="252"><span class="lineNum">     252 </span>            : </a>
<a name="253"><span class="lineNum">     253 </span><span class="lineCov">         46 :         cv::Mat mapx, mapy;</span></a>
<a name="254"><span class="lineNum">     254 </span><span class="lineCov">         69 :         cv::initUndistortRectifyMap(</span></a>
<a name="255"><span class="lineNum">     255 </span><span class="lineCov">         46 :                 cameraMatrix, distCoeffs, cv::Mat(), cameraMatrix,</span></a>
<a name="256"><span class="lineNum">     256 </span>            :                 cv::Size(1280, 720),</a>
<a name="257"><span class="lineNum">     257 </span>            :                 CV_32FC1, mapx, mapy</a>
<a name="258"><span class="lineNum">     258 </span>            :         );  // Compute undistortion mapping</a>
<a name="259"><span class="lineNum">     259 </span>            : </a>
<a name="260"><span class="lineNum">     260 </span><span class="lineCov">         23 :         d_mapx.upload(mapx);  // Upload X map to GPU</span></a>
<a name="261"><span class="lineNum">     261 </span><span class="lineCov">         23 :         d_mapy.upload(mapy);  // Upload Y map to GPU</span></a>
<a name="262"><span class="lineNum">     262 </span>            : }</a>
<a name="263"><span class="lineNum">     263 </span>            : </a>
<a name="264"><span class="lineNum">     264 </span><span class="lineNoCov">          0 : CenterlineResult TensorRTInferencer::getPolyfittingResult(const cv::cuda::GpuMat&amp; processedMaskGpu) {</span></a>
<a name="265"><span class="lineNum">     265 </span>            :         // Download the processed mask to CPU for lane fitting</a>
<a name="266"><span class="lineNum">     266 </span><span class="lineNoCov">          0 :         cv::Mat maskCpu;</span></a>
<a name="267"><span class="lineNum">     267 </span><span class="lineNoCov">          0 :         processedMaskGpu.download(maskCpu);</span></a>
<a name="268"><span class="lineNum">     268 </span>            : </a>
<a name="269"><span class="lineNum">     269 </span>            :         // Fit lanes and compute centerline</a>
<a name="270"><span class="lineNum">     270 </span><span class="lineNoCov">          0 :         auto centerlineOpt = laneCurveFitter-&gt;computeCenterline(maskCpu);</span></a>
<a name="271"><span class="lineNum">     271 </span>            : </a>
<a name="272"><span class="lineNum">     272 </span><span class="lineNoCov">          0 :         if (centerlineOpt.has_value()) {</span></a>
<a name="273"><span class="lineNum">     273 </span><span class="lineNoCov">          0 :                 return centerlineOpt.value();</span></a>
<a name="274"><span class="lineNum">     274 </span>            :         } </a>
<a name="275"><span class="lineNum">     275 </span>            :         else {</a>
<a name="276"><span class="lineNum">     276 </span>            :                 // std::cerr &lt;&lt; &quot;[Error] Failed to compute centerline from mask&quot; &lt;&lt; std::endl;</a>
<a name="277"><span class="lineNum">     277 </span><span class="lineNoCov">          0 :                 return CenterlineResult();  // Return empty result if fitting fails</span></a>
<a name="278"><span class="lineNum">     278 </span>            :         }</a>
<a name="279"><span class="lineNum">     279 </span>            : }</a>
<a name="280"><span class="lineNum">     280 </span>            : </a>
<a name="281"><span class="lineNum">     281 </span><span class="lineNoCov">          0 : void TensorRTInferencer::doInference(const cv::Mat&amp; frame) {</span></a>
<a name="282"><span class="lineNum">     282 </span><span class="lineNoCov">          0 :         if (frame.empty()) {</span></a>
<a name="283"><span class="lineNum">     283 </span><span class="lineNoCov">          0 :                 throw std::runtime_error(&quot;Input frame is empty&quot;);</span></a>
<a name="284"><span class="lineNum">     284 </span>            :         }</a>
<a name="285"><span class="lineNum">     285 </span>            : </a>
<a name="286"><span class="lineNum">     286 </span><span class="lineNoCov">          0 :         cv::cuda::GpuMat d_frame(frame);  // Upload frame to GPU</span></a>
<a name="287"><span class="lineNum">     287 </span><span class="lineNoCov">          0 :         cv::cuda::GpuMat d_undistorted;</span></a>
<a name="288"><span class="lineNum">     288 </span><span class="lineNoCov">          0 :         cv::cuda::remap(d_frame, d_undistorted, d_mapx, d_mapy, cv::INTER_LINEAR, 0, cv::Scalar(), cudaStream);  // Undistort frame</span></a>
<a name="289"><span class="lineNum">     289 </span>            : </a>
<a name="290"><span class="lineNum">     290 </span><span class="lineNoCov">          0 :         cv::cuda::GpuMat d_prediction_mask = makePrediction(d_undistorted);  // Run model inference</span></a>
<a name="291"><span class="lineNum">     291 </span>            : </a>
<a name="292"><span class="lineNum">     292 </span>            :         // Convert to 8-bit (0 or 255) in a new GpuMat</a>
<a name="293"><span class="lineNum">     293 </span><span class="lineNoCov">          0 :         cv::cuda::GpuMat d_mask_u8;</span></a>
<a name="294"><span class="lineNum">     294 </span><span class="lineNoCov">          0 :         d_prediction_mask.convertTo(d_mask_u8, CV_8U, 255.0);  // Multiply 0/1 float to 0/255</span></a>
<a name="295"><span class="lineNum">     295 </span>            : </a>
<a name="296"><span class="lineNum">     296 </span>            :         // ---- Post-process (GPU) ----</a>
<a name="297"><span class="lineNum">     297 </span><span class="lineNoCov">          0 :     cv::cuda::GpuMat d_postprocessed;</span></a>
<a name="298"><span class="lineNum">     298 </span><span class="lineNoCov">          0 :     d_postprocessed = lanePostProcessor-&gt;process(d_prediction_mask);  // Post-process the prediction mask</span></a>
<a name="299"><span class="lineNum">     299 </span>            : </a>
<a name="300"><span class="lineNum">     300 </span>            :         // ---- Fit lanes and compute centerline ----</a>
<a name="301"><span class="lineNum">     301 </span><span class="lineNoCov">          0 :         CenterlineResult centerlineResult = getPolyfittingResult(d_postprocessed);  // Get centerline result</span></a>
<a name="302"><span class="lineNum">     302 </span>            : </a>
<a name="303"><span class="lineNum">     303 </span><span class="lineNoCov">          0 :         Publisher::instance(INFERENCE_PORT)-&gt;publishInferenceFrame(INFERENCE_TOPIC, d_mask_u8); //Publish frame to ZeroMQ publisher</span></a>
<a name="304"><span class="lineNum">     304 </span>            : </a>
<a name="305"><span class="lineNum">     305 </span><span class="lineNoCov">          0 :         Publisher::instance(POLYFITTING_PORT)-&gt;publishPolyfittingResult(POLYFITTING_TOPIC, centerlineResult); // Publish centerline result to ZeroMQ publisher</span></a>
<a name="306"><span class="lineNum">     306 </span><span class="lineNoCov">          0 : }</span></a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.14</a></td></tr>
  </table>
  <br>

</body>
</html>
